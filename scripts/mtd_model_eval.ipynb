{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "def get_imagenet12_ds(transform: Optional[callable] = None, batch_size: int = 32, shuffle: bool = True, num_workers: int = 16):\n",
    "    imagenet_path = '/home/ran/datasets/imagenet12'\n",
    "\n",
    "    def _default_transform(img):\n",
    "        if not isinstance(img, torch.Tensor):\n",
    "            img = torchvision.transforms.ToTensor()(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    if transform is None:\n",
    "        transform = _default_transform\n",
    "\n",
    "    imagenet_data = torchvision.datasets.ImageNet(imagenet_path, split='val', transform=transform)\n",
    "    data_loader = torch.utils.data.DataLoader(imagenet_data,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=shuffle,\n",
    "                                            num_workers=num_workers)\n",
    "    \n",
    "    return data_loader\n",
    "\n",
    "def eval_torch_model(model, data, use_cuda:bool = True, transform: Optional[callable] = None, n_subset: Optional[int] = None):\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if cuda_available and use_cuda else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(data):\n",
    "            if n_subset and i>=n_subset:\n",
    "                break\n",
    "\n",
    "            if transform:\n",
    "                images = transform(images)\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Get the predicted classes\n",
    "            _, predicted_top1 = torch.max(outputs.data, 1)\n",
    "            _, predicted_top5 = torch.topk(outputs, 5, dim=1)\n",
    "            \n",
    "            # Update the accuracy metrics\n",
    "            total += labels.size(0)\n",
    "            correct_top1 += (predicted_top1 == labels).sum().item()\n",
    "            correct_top5 += (labels.unsqueeze(1) == predicted_top5).any(1).sum().item()\n",
    "\n",
    "    # Calculate and print the overall accuracy\n",
    "    accuracy_top1 = 100 * correct_top1 / total\n",
    "    accuracy_top5 = 100 * correct_top5 / total\n",
    "\n",
    "    return accuracy_top1, accuracy_top5\n",
    "\n",
    "weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "ds = get_imagenet12_ds(transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy on the ImageNet validation set: 75.00%\n",
      "Top-5 Accuracy on the ImageNet validation set: 96.88%\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(weights=weights)\n",
    "\n",
    "accuray_top1, accuracy_top5 = eval_torch_model(model, ds, n_subset=1)\n",
    "\n",
    "print(f'Top-1 Accuracy on the ImageNet validation set: {accuray_top1:.2f}%')\n",
    "print(f'Top-5 Accuracy on the ImageNet validation set: {accuracy_top5:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexnet\n",
      "convnext_base\n",
      "convnext_large\n",
      "convnext_small\n",
      "convnext_tiny\n",
      "deeplabv3_mobilenet_v3_large\n",
      "deeplabv3_resnet101\n",
      "deeplabv3_resnet50\n",
      "densenet121\n",
      "densenet161\n",
      "densenet169\n",
      "densenet201\n",
      "efficientnet_b0\n",
      "efficientnet_b1\n",
      "efficientnet_b2\n",
      "efficientnet_b3\n",
      "efficientnet_b4\n",
      "efficientnet_b5\n",
      "efficientnet_b6\n",
      "efficientnet_b7\n",
      "efficientnet_v2_l\n",
      "efficientnet_v2_m\n",
      "efficientnet_v2_s\n",
      "fasterrcnn_mobilenet_v3_large_320_fpn\n",
      "fasterrcnn_mobilenet_v3_large_fpn\n",
      "fasterrcnn_resnet50_fpn\n",
      "fasterrcnn_resnet50_fpn_v2\n",
      "fcn_resnet101\n",
      "fcn_resnet50\n",
      "fcos_resnet50_fpn\n",
      "googlenet\n",
      "inception_v3\n",
      "keypointrcnn_resnet50_fpn\n",
      "lraspp_mobilenet_v3_large\n",
      "maskrcnn_resnet50_fpn\n",
      "maskrcnn_resnet50_fpn_v2\n",
      "maxvit_t\n",
      "mc3_18\n",
      "mnasnet0_5\n",
      "mnasnet0_75\n",
      "mnasnet1_0\n",
      "mnasnet1_3\n",
      "mobilenet_v2\n",
      "mobilenet_v3_large\n",
      "mobilenet_v3_small\n",
      "mvit_v1_b\n",
      "mvit_v2_s\n",
      "quantized_googlenet\n",
      "quantized_inception_v3\n",
      "quantized_mobilenet_v2\n",
      "quantized_mobilenet_v3_large\n",
      "quantized_resnet18\n",
      "quantized_resnet50\n",
      "quantized_resnext101_32x8d\n",
      "quantized_resnext101_64x4d\n",
      "quantized_shufflenet_v2_x0_5\n",
      "quantized_shufflenet_v2_x1_0\n",
      "quantized_shufflenet_v2_x1_5\n",
      "quantized_shufflenet_v2_x2_0\n",
      "r2plus1d_18\n",
      "r3d_18\n",
      "raft_large\n",
      "raft_small\n",
      "regnet_x_16gf\n",
      "regnet_x_1_6gf\n",
      "regnet_x_32gf\n",
      "regnet_x_3_2gf\n",
      "regnet_x_400mf\n",
      "regnet_x_800mf\n",
      "regnet_x_8gf\n",
      "regnet_y_128gf\n",
      "regnet_y_16gf\n",
      "regnet_y_1_6gf\n",
      "regnet_y_32gf\n",
      "regnet_y_3_2gf\n",
      "regnet_y_400mf\n",
      "regnet_y_800mf\n",
      "regnet_y_8gf\n",
      "resnet101\n",
      "resnet152\n",
      "resnet18\n",
      "resnet34\n",
      "resnet50\n",
      "resnext101_32x8d\n",
      "resnext101_64x4d\n",
      "resnext50_32x4d\n",
      "retinanet_resnet50_fpn\n",
      "retinanet_resnet50_fpn_v2\n",
      "s3d\n",
      "shufflenet_v2_x0_5\n",
      "shufflenet_v2_x1_0\n",
      "shufflenet_v2_x1_5\n",
      "shufflenet_v2_x2_0\n",
      "squeezenet1_0\n",
      "squeezenet1_1\n",
      "ssd300_vgg16\n",
      "ssdlite320_mobilenet_v3_large\n",
      "swin3d_b\n",
      "swin3d_s\n",
      "swin3d_t\n",
      "swin_b\n",
      "swin_s\n",
      "swin_t\n",
      "swin_v2_b\n",
      "swin_v2_s\n",
      "swin_v2_t\n",
      "vgg11\n",
      "vgg11_bn\n",
      "vgg13\n",
      "vgg13_bn\n",
      "vgg16\n",
      "vgg16_bn\n",
      "vgg19\n",
      "vgg19_bn\n",
      "vit_b_16\n",
      "vit_b_32\n",
      "vit_h_14\n",
      "vit_l_16\n",
      "vit_l_32\n",
      "wide_resnet101_2\n",
      "wide_resnet50_2\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "for name in models.list_models():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtd-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
